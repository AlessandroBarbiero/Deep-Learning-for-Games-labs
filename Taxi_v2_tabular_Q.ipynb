{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlessandroBarbiero/deep-learning-for-games-labs/blob/main/Taxi_v2_tabular_Q.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iMs6H6nlc7H"
      },
      "source": [
        "\n",
        "# Q Learning with OpenAI Taxi-v3 ðŸ•¹ï¸ðŸš•\n",
        "\n",
        "\n",
        "In this Notebook, we'll implement an agent that plays OpenAI Taxi-v3.\n",
        "\n",
        "The goal of this game is that our agent must pick up the passenger at one location and drop him off to the goal as fast as possible.\n",
        "\n",
        "There are 4 locations (labeled by different letters) and your job is to pick up the passenger at one location and drop him off in another.\n",
        "\n",
        "* You receive +20 points for a successful dropoff\n",
        "* Lose 1 point for every timestep it takes.\n",
        "* There is also a 10 point penalty for illegal pick-up and drop-off actions (if you don't drop the passenger in one of the 3 other locations)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSYdY5GEly4z"
      },
      "source": [
        "## Step 0: Import libs\n",
        "\n",
        "\n",
        "* Numpy for our Qtable\n",
        "* OpenAI Gym for our Taxi Environment\n",
        "* Random to generate random numbers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "4V_yqRnLc4U7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5uEXNcRl5ep"
      },
      "source": [
        "## Step 1: Initiate the environment\n",
        "\n",
        "\n",
        "* Here we'll create the Taxi environment.\n",
        "* OpenAI Gym is a library composed of many environments that we can use to train our agents.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNkGl_7rdZUL",
        "outputId": "4c30cf12-c49d-45a7-a9b5-4593ca8898d1"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"Taxi-v3\", render_mode='human')\n",
        "env.reset()\n",
        "env.render()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93dq5V1bmGYr"
      },
      "source": [
        "## Step 2: Create a Q-table and init it with zeros\n",
        "\n",
        "\n",
        "* Now, we'll create our Q-table, to know how much rows (states) and columns (actions) we need, we need to calculate the `action_size` and the `state_size`\n",
        "* OpenAI Gym provides us a way to do that: `env.action_space.n` and `env.observation_space.n`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gWI6T9gJdkze",
        "outputId": "30f540f8-ea7c-4ab4-f225-5823fc1bbe1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Action size  6\n",
            "State size  500\n",
            "Discrete(6)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "action_size = env.action_space.n\n",
        "print(\"Action size \", action_size)\n",
        "\n",
        "state_size = env.observation_space.n\n",
        "print(\"State size \", state_size)\n",
        "\n",
        "obs = env.reset()\n",
        "print(env.action_space)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOVmgktPdr68",
        "outputId": "efeedd8d-9acf-4889-d60d-c035a84c5251"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]]\n"
          ]
        }
      ],
      "source": [
        "qtable = np.zeros((state_size, action_size))\n",
        "print(qtable)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1dtCv_pmfMr"
      },
      "source": [
        "## Step 3: Create the hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "hx9Zig-3dtaQ"
      },
      "outputs": [],
      "source": [
        "total_episodes = 100        # Total episodes //50000\n",
        "total_test_episodes = 5     # Total test episodes\n",
        "max_steps = 99                # Max steps per episode  //99\n",
        "\n",
        "learning_rate = 0.7           # Learning rate\n",
        "gamma = 0.618                 # Discounting rate\n",
        "\n",
        "# Exploration parameters\n",
        "epsilon = 1.0                 # Exploration rate\n",
        "max_epsilon = 1.0             # Exploration probability at start\n",
        "min_epsilon = 0.01            # Minimum exploration probability \n",
        "decay_rate = 0.01             # Exponential decay rate for exploration prob"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aSlBGIHmmG2"
      },
      "source": [
        "## Step 4: The Q learning algorithm\n",
        "\n",
        "* Here we implement the Q-learning algorithm that will train the taxi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "WplTBEYidyCL"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "episode return -360\n",
            "episode return -387\n",
            "episode return -378\n",
            "episode return -351\n",
            "episode return -342\n",
            "episode return -369\n",
            "episode return -432\n",
            "episode return -360\n",
            "episode return -387\n",
            "episode return -405\n",
            "episode return -378\n",
            "episode return -288\n",
            "episode return -351\n",
            "episode return -360\n",
            "episode return -351\n",
            "episode return -405\n",
            "episode return -369\n",
            "episode return -360\n",
            "episode return -279\n",
            "episode return -369\n",
            "episode return -333\n",
            "episode return -441\n",
            "episode return -378\n",
            "episode return -315\n",
            "episode return -279\n",
            "episode return -351\n",
            "episode return -342\n",
            "episode return -414\n",
            "episode return -351\n",
            "episode return -288\n",
            "episode return -342\n",
            "episode return -423\n",
            "episode return -324\n",
            "episode return -324\n",
            "episode return -378\n",
            "episode return -288\n",
            "episode return -333\n",
            "episode return -342\n",
            "episode return -369\n",
            "episode return -324\n",
            "episode return -288\n",
            "episode return -315\n",
            "episode return -306\n",
            "episode return -324\n",
            "episode return -279\n",
            "episode return -369\n",
            "episode return -297\n",
            "episode return -387\n",
            "episode return -324\n",
            "episode return -324\n",
            "episode return -297\n",
            "episode return -387\n",
            "episode return -288\n",
            "episode return -342\n",
            "episode return -288\n",
            "episode return -261\n",
            "episode return -297\n",
            "episode return -333\n",
            "episode return -288\n",
            "episode return -279\n",
            "episode return -234\n",
            "episode return -270\n",
            "episode return -324\n",
            "episode return -243\n",
            "episode return -297\n",
            "episode return -252\n",
            "episode return -306\n",
            "episode return -270\n",
            "episode return -315\n",
            "episode return -198\n",
            "episode return -297\n",
            "episode return -261\n",
            "episode return -261\n",
            "episode return -171\n",
            "episode return -261\n",
            "episode return -270\n",
            "episode return -270\n",
            "episode return -270\n",
            "episode return -216\n",
            "episode return -243\n",
            "episode return -279\n",
            "episode return -234\n",
            "episode return -324\n",
            "episode return -225\n",
            "episode return -234\n",
            "episode return -261\n",
            "episode return -243\n",
            "episode return -225\n",
            "episode return -180\n",
            "episode return -234\n",
            "episode return -243\n",
            "episode return -243\n",
            "episode return -198\n",
            "episode return -216\n",
            "episode return -199\n",
            "episode return -279\n",
            "episode return -252\n",
            "episode return -266\n",
            "episode return -198\n",
            "episode return -180\n"
          ]
        }
      ],
      "source": [
        "# 2 For life or until learning is stopped\n",
        "for episode in range(total_episodes):\n",
        "    # Reset the environment\n",
        "    state,_ = env.reset()\n",
        "    step = 0\n",
        "    done = False\n",
        "    total_return = 0\n",
        "    for step in range(max_steps):\n",
        "        # 3. Choose an action a in the current world state (s)\n",
        "        ## First we randomize a number\n",
        "        exp_exp_tradeoff = random.uniform(0,1)\n",
        "        \n",
        "        ## If this number > greater than epsilon --> exploitation (taking the biggest Q value for this state)\n",
        "        if exp_exp_tradeoff > epsilon:\n",
        "            action = np.argmax(qtable[state,:])\n",
        "        \n",
        "        # Else doing a random choice --> exploration\n",
        "        else:\n",
        "            action = env.action_space.sample()\n",
        "        \n",
        "        # Take the action (a) and observe the outcome state(s') and reward (r)\n",
        "        # step function changed in the api adding a truncate value, and the one that here is new_state can be intended as observation \n",
        "        new_state, reward, done, truncate, info = env.step(action)\n",
        "\n",
        "        # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
        "        qtable[state, action] = qtable[state, action] + learning_rate * (reward + gamma * \n",
        "                                    np.max(qtable[new_state, :]) - qtable[state, action])\n",
        "                \n",
        "        # Our new state is state\n",
        "        state = new_state\n",
        "        \n",
        "        total_return += reward\n",
        "        # If done : finish episode\n",
        "        if done == True: \n",
        "            break\n",
        "    print(f\"episode return {total_return}\")\n",
        "    \n",
        "    # Reduce epsilon (because we need less and less exploration)\n",
        "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPkVzuj0eeFn"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJ9SAixtmv-1"
      },
      "source": [
        "## Step 5: Use our Q-table to define the Taxi driving policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bJQB5Nrd8W1",
        "outputId": "cdba12b6-1057-48ad-ac58-b86d8a0f9795"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "****************************************************\n",
            "EPISODE  0\n",
            "****************************************************\n",
            "EPISODE  1\n",
            "****************************************************\n",
            "EPISODE  2\n",
            "****************************************************\n",
            "EPISODE  3\n",
            "****************************************************\n",
            "EPISODE  4\n",
            "Score over time: 0.0\n"
          ]
        }
      ],
      "source": [
        "env.reset()\n",
        "rewards = []\n",
        "\n",
        "for episode in range(total_test_episodes):\n",
        "    state,_ = env.reset()\n",
        "    ##img = env.render(mode=\"rgb_array\")\n",
        "    step = 0\n",
        "    done = False\n",
        "    total_rewards = 0\n",
        "    print(\"****************************************************\")\n",
        "    print(\"EPISODE \", episode)\n",
        "\n",
        "    for step in range(max_steps):\n",
        "        ## Render images and save into an array\n",
        "        #images.append(img)\n",
        "        img = env.render()\n",
        "\n",
        "        action = np.argmax(qtable[state,:])\n",
        "        \n",
        "        new_state, reward, done, truncate, info = env.step(action)\n",
        "        \n",
        "        total_rewards += reward\n",
        "        \n",
        "        if done:\n",
        "            rewards.append(total_rewards)\n",
        "            print (\"Score\", total_rewards)\n",
        "            break\n",
        "        state = new_state\n",
        "env.close()\n",
        "#imageio.mimwrite('./taxi.gif',\n",
        "#                [np.array(img) for i, img in enumerate(images) if i%2 == 0],\n",
        "#                fps=29)\n",
        "print (\"Score over time: \" +  str(sum(rewards)/total_test_episodes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "6XW6WhZjgVqA"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "gifPath = Path(\"./taxi.gif\")\n",
        "# Display GIF in Jupyter, CoLab, IPython\n",
        "# with open(gifPath,'rb') as f:\n",
        "#     display.Image(data=f.read(), format='png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUaWxEBKz1UQ",
        "outputId": "351588f4-dba1-4011-fe70-7514d2446132"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[-1.70121951 -1.58851193 -1.69049565 -1.58851193 -1.57372054 -7.        ]\n"
          ]
        }
      ],
      "source": [
        "print(qtable[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Save the Q-table in a txt file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "np.savetxt(\"qtable\", qtable)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 ('dlgs')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "aa87fa5d0095d65c3d05fa3a4de96fff1a1d333803f7fbe0adaa457ad4e6feb1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
