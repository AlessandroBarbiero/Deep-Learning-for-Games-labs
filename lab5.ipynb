{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Deep Reinforcement Learning\n",
    "In this lab we will implement and train an agent that uses deep learning to play balance the stick in `CartPole-v1`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Setup\n",
    "----\n",
    "We import useful packages: `gym`, `torch` stuff, etc.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from collections import deque  # for memory\n",
    "from tqdm import tqdm          # for progress bar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "How the game looks (without our agent):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def rnd_game():\n",
    "    env = gym.make('CartPole-v1', render_mode='human')\n",
    "    for _ in tqdm(range(10)):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = env.action_space.sample()\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "    env.close()\n",
    "\n",
    "# rnd_game()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## DQN Algorithm\n",
    "-------------\n",
    "We train a policy that tries to maximize the discounted,\n",
    "cumulative reward\n",
    "$R_{t_0} = \\sum_{t=t_0}^{\\infty} \\gamma^{t - t_0} r_t$, where\n",
    "$R_{t_0}$ is *return*. The discount, $\\gamma$ is the discount, between $0$ and $1$\n",
    "\n",
    "\n",
    "Q-learning tries to find a function\n",
    "$Q^*: State \\times Action \\rightarrow \\mathbb{R}$, maximizes rewards:\n",
    "\n",
    "$ \\begin{align}\\pi^*(s) = \\arg\\!\\max_a \\ Q^*(s, a)\\end{align} $\n",
    "\n",
    "However, we don't know $ Q^* $. So, we use neural network as a approximators, we can simply create one and train it to resemble $ Q^* $.\n",
    "\n",
    "For our training update rule, we'll use a fact that every $ Q $\n",
    "function for some policy obeys the Bellman equation:\n",
    "\n",
    "$ \\begin{align}Q^{\\pi}(s, a) = r + \\gamma Q^{\\pi}(s', \\pi(s'))\\end{align} $\n",
    "\n",
    "The difference between the two sides of the equality is known as the temporal difference error, $ \\delta $:\n",
    "\n",
    "$ \\begin{align}\\delta = Q(s, a) - (r + \\gamma \\max_a Q(s', a))\\end{align} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Model\n",
    "---\n",
    "Make a deep learning based policy model, that takes in a state and outputs an action.\n",
    "This model will be an attribute of the Agent we make next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, observation_size, action_size):\n",
    "        super(Model, self).__init__()\n",
    "        # initialise layers here\n",
    "        self.layer1 = nn.Linear(observation_size, 10)    # create dense layer 1\n",
    "        self.layer2 = nn.Linear(10, action_size)         # create dense layer 2\n",
    " \n",
    "    # uso il forward solo con x che è una lista di interi o float\n",
    "    def forward(self, x):\n",
    "        # send x through the network\n",
    "\n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        x = self.layer1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def predictBestReward(self, x):\n",
    "        x = self.forward(x)            # send x through neural net\n",
    "        # res,_ = torch.max(x, dim=0)\n",
    "        return torch.max(x)     # predict the best reward\n",
    "\n",
    "    def predictBestAction(self, x):\n",
    "        x = self.forward(x)               # send x through neural net\n",
    "        return torch.argmax(x, dim=0)     # predict the best action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### DQN Agent\n",
    "----\n",
    "We will be using experience replay memory for training our model.\n",
    "An Agent's memory is as important as its model, and will be another attribute of our agent.\n",
    "Other appropriate attributes are the hyperparameters (gamma, lr, etc.).\n",
    "Give the agent a replay method that trains on a batch from its memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, observation_size, action_size):\n",
    "\n",
    "        self.observation_size=observation_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.model = Model(observation_size, action_size)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=1e-3)\n",
    "\n",
    "        # memory that stores N most new transitions\n",
    "        self.memory_size = 1000\n",
    "        self.memory = deque()\n",
    "        self.memory_full = False\n",
    "        \n",
    "        # good place to store hyperparameters as attributes\n",
    "        self.gamma = 0.9\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        sars = {\"state\": state,\n",
    "                \"action\": action,\n",
    "                \"reward\": reward,\n",
    "                \"next_state\": next_state,\n",
    "                \"done\": done}\n",
    "        # add to memory\n",
    "        self.memory.append(sars)\n",
    "        # remove oldest value if the memory is full\n",
    "        if self.memory_full:\n",
    "            self.memory.popleft()\n",
    "        elif len(self.memory) == self.memory_size :\n",
    "            self.memory_full = True\n",
    "\n",
    "    def act(self, state):\n",
    "        # return an action from the model\n",
    "        # out = self.model.forward(state)\n",
    "        # return torch.argmax(out, dim=0)\n",
    "        return self.model.predictBestAction(state)\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        # update model based on replay memory\n",
    "        # you might want to make a self.train() helper method\n",
    "        batch = random.sample(self.memory, batch_size)\n",
    "        for sars in batch:\n",
    "            self.train(sars)\n",
    "\n",
    "    def train(self, sars):\n",
    "        \n",
    "        self.optimizer.zero_grad()                                # clean gradients of parameters\n",
    "\n",
    "        pred = self.model.forward(sars[\"state\"])[sars[\"action\"]]        # take the Q value of the action\n",
    "\n",
    "        if sars[\"done\"] == True:\n",
    "            y = sars[\"reward\"]\n",
    "            y = torch.tensor(y, dtype = torch.float32)\n",
    "        else:\n",
    "            max_reward = self.model.predictBestReward(sars[\"next_state\"])\n",
    "            #out_tensor = self.model.forward(sars[\"next_state\"])\n",
    "            # The maximum of the rewards achievable from the next_state, \n",
    "            # with argmax I take the best action, with max the reward of the best action\n",
    "            #max_value = torch.max(out_tensor).item() \n",
    "            y = sars[\"reward\"] + self.gamma * max_reward\n",
    "        loss = self.criterion(pred, y)          # calculate loss with respect to prediction\n",
    "        loss.backward()                         # calculate gradients of model.parameters() with respect to loss\n",
    "        self.optimizer.step()                   # update parameters with respect to gradients\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Main Training loop\n",
    "---\n",
    "Make a function that takes an environment and an agent, and runs through $ n $ episodes.\n",
    "Remember to call that agent's replay function to learn from its past (once it has a past).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(env, agent: Agent, episodes=1000, batch_size=64):  # train for many games\n",
    "    for _ in tqdm(range(episodes)):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            # 1. make a move in game.\n",
    "            tradeoff = random.uniform(0,1)\n",
    "            if tradeoff > 0.3:\n",
    "                action = agent.model.predictBestAction(state).item()\n",
    "            else:\n",
    "                action = env.action_space.sample()\n",
    "            # Take the action (a) and observe the outcome state(s') and reward (r)\n",
    "            new_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "            # 2. have the agent remember stuff.\n",
    "            agent.remember(state, action, reward, new_state, done)\n",
    "\n",
    "            # 3. update state\n",
    "            state = new_state\n",
    "\n",
    "            # 4. if we have enough experiences in our memory, learn from a batch with replay.\n",
    "            if len(agent.memory) >= batch_size:\n",
    "                agent.replay(batch_size)\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Putting it together\n",
    "---\n",
    "We train an agent on the environment:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 13/1000 [00:35<44:21,  2.70s/it] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2148\\1408495227.py\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'CartPole-v1'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrender_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'human'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0magent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAgent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'modelCartPole.pth'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2148\\3099127802.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(env, agent, episodes, batch_size)\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[1;31m# 4. if we have enough experiences in our memory, learn from a batch with replay.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m                 \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m     \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2148\\1387902973.py\u001b[0m in \u001b[0;36mreplay\u001b[1;34m(self, batch_size)\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0msars\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2148\\1387902973.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, sars)\u001b[0m\n\u001b[0;32m     54\u001b[0m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m             \u001b[0mmax_reward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredictBestReward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msars\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"next_state\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m             \u001b[1;31m#out_tensor = self.model.forward(sars[\"next_state\"])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m             \u001b[1;31m# The maximum of the rewards achievable from the next_state,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2148\\2332322051.py\u001b[0m in \u001b[0;36mpredictBestReward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m            \u001b[1;31m# send x through neural net\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;31m# res,_ = torch.max(x, dim=0)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m     \u001b[1;31m# predict the best reward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredictBestAction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1', render_mode='human')\n",
    "agent = Agent(env.observation_space.shape[0], env.action_space.n)\n",
    "train(env, agent)\n",
    "torch.save(agent.model.state_dict(), 'modelCartPole.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Optional (highly recommended): Atari\n",
    "Adapt your agent to play an Atari game of your choice.\n",
    "https://www.gymlibrary.dev/environments/atari/air_raid/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('dlgsVenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "245e500c7ea10f8a0a5166b0fee45606f08f8f8cd109f252cc77539db3006236"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
