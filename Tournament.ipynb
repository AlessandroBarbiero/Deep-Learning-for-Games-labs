{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ale/dlgs/tournamentVenv/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pettingzoo.atari import surround_v2\n",
    "import supersuit\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display as ipythondisplay\n",
    "from pyvirtualdisplay import Display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(env, agent, action, idx):\n",
    "    screen = env.render()\n",
    "    plt.imshow(screen)\n",
    "    plt.show()\n",
    "    #plt.title(f\"{idx}, {agent}, {action}\")\n",
    "\n",
    "def get_env():\n",
    "    env = surround_v2.env(render_mode=\"human\")\n",
    "    env = supersuit.max_observation_v0(env, 2)\n",
    "    # env = supersuit.sticky_actions_v0(env, repeat_action_probability=0.25)\n",
    "    env = supersuit.frame_skip_v0(env, 4)\n",
    "    env = supersuit.resize_v1(env, 84, 84)\n",
    "    # env = supersuit.frame_stack_v1(env, 4)\n",
    "    return env\n",
    "\n",
    "\n",
    "# display = Display(visible=0, size=(400, 300))\n",
    "# display.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, env, agent):\n",
    "        self.action_space = env.action_space(agent)\n",
    "        self.observation_space = env.observation_space(agent)\n",
    "        self.agent = agent\n",
    "\n",
    "    def get_action(self, obs):\n",
    "        pass\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        pass\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        pass\n",
    "\n",
    "    def canReplay(self, batch_size):\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent(Agent):\n",
    "    def __init__(self, env, agent):\n",
    "        super(RandomAgent, self).__init__(env, agent)\n",
    "\n",
    "    def get_action(self, obs):\n",
    "        return self.action_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepReinforcementLearning Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, action_space: int, observation_space):\n",
    "        super(Model, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels = 3, out_channels = 32, kernel_size = 3, stride = 1)  \n",
    "        self.conv2 = nn.Conv2d(in_channels = 32, out_channels = 3, kernel_size = 2, stride = 1)  \n",
    "\n",
    "        self.dense1 = nn.Linear(81*81*3, 1000)    \n",
    "        self.dense2 = nn.Linear(1000, 300)    \n",
    "        self.dense3 = nn.Linear(300, action_space)     \n",
    "\n",
    "    def forward(self, x): \n",
    "        x = self.conv1(x)   \n",
    "        x = F.relu(x)      \n",
    "        x = self.conv2(x)              \n",
    "        x = x.flatten()\n",
    "        x = self.dense1(x)             \n",
    "        x = F.relu(x)                  \n",
    "        x = self.dense2(x) \n",
    "        x = F.relu(x)                  \n",
    "        x = self.dense3(x) \n",
    "        return x\n",
    "\n",
    "    def predict(self, x):              \n",
    "        x = self.forward(x)     \n",
    "        return torch.argmax(x, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlebaGiogAgent(Agent):\n",
    "    def __init__(self, env, agent):\n",
    "        super(AlebaGiogAgent, self).__init__(env, agent)\n",
    "        self.model = Model(action_space = env.action_space(agent).n, observation_space = env.observation_space(agent))\n",
    "\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=1e-3)\n",
    "\n",
    "        self.memory = deque()\n",
    "\n",
    "        self.gamma = 0.9\n",
    "        self.eps = 1\n",
    "        self.max_eps = 1.0\n",
    "        self.min_eps = 0.01\n",
    "        self.decay_rate = 0.001\n",
    "\n",
    "    def get_action(self, obs):\n",
    "        obs = self.preprocess_obs(obs)\n",
    "        return self.model.predict(obs).item()\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        if len(self.memory) >= 1000:\n",
    "            self.memory.popleft()\n",
    "        self.memory.append({\"state\" : self.preprocess_obs(state), \n",
    "                            \"action\" : action, \n",
    "                            \"reward\" : reward, \n",
    "                            \"next_state\" : self.preprocess_obs(next_state),\n",
    "                            \"done\" : done})\n",
    "\n",
    "    def preprocess_obs(self, obs):\n",
    "        obs = torch.tensor(obs, dtype=torch.double) / 255\n",
    "        obs = obs.view(-1, 3, 84, 84)\n",
    "        return obs\n",
    "\n",
    "    def canReplay(self, batch_size):\n",
    "        return len(self.memory) >= batch_size\n",
    "\n",
    "    def replay(self, batch_size=64):\n",
    "        loader = DataLoader(self.memory, batch_size=batch_size, shuffle=True)\n",
    "        self.train(loader)\n",
    "\n",
    "    def train(self,loader):\n",
    "        for sars in loader:                         # loop through batches\n",
    "            # print(sars[\"state\"].shape)\n",
    "            self.optimizer.zero_grad()              # clean gradients of parameters\n",
    "            pred = self.model(sars[\"state\"][0])           # make prediction\n",
    "            if sars[\"done\"][0]:\n",
    "                y = sars[\"reward\"][0]\n",
    "            else: \n",
    "                y = sars[\"reward\"][0] + self.gamma * torch.max(self.model(sars[\"next_state\"][0]))\n",
    "            loss = self.criterion(pred, y)          # calculate loss with respect to prediction\n",
    "            loss.backward()                         # calculate gradients of model.parameters() with respect to loss\n",
    "            self.optimizer.step() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = get_env()\n",
    "env.reset()\n",
    "\n",
    "agents = {\n",
    "    env.agents[0] : AlebaGiogAgent(env, env.agents[0]),\n",
    "    env.agents[1] : RandomAgent(env, env.agents[1])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, episodes, batch_size = 64):\n",
    "    epsilon = 1\n",
    "    for i in tqdm(range(episodes)):\n",
    "        epsilon -= 1/episodes\n",
    "        env.reset()\n",
    "        for agent_id in env.agent_iter():\n",
    "            agent = agents[agent_id]\n",
    "            # 1. Update state\n",
    "            state, _, done, _, _ = env.last()\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            # 2. Make a move in game.\n",
    "            tradeoff = random.uniform(0,1)\n",
    "            if tradeoff > epsilon:\n",
    "                action = agent.get_action(state)\n",
    "            else:\n",
    "                action = agent.action_space.sample()\n",
    "            # Take the action (a) and observe the outcome state(s') and reward (r)\n",
    "            env.step(action)\n",
    "            new_state, reward, done, _, _ = env.last()\n",
    "            # 3. Have the agent remember stuff.\n",
    "            agent.remember(state, action, reward, new_state, done)\n",
    "\n",
    "            # 4. if we have enough experiences in our memory, learn from a batch with replay.\n",
    "            if agent.canReplay(batch_size):\n",
    "                agent.replay(batch_size)\n",
    "            \n",
    "\n",
    "# for agent_id in env.agent_iter():\n",
    "#     agent = agents[agent_id]\n",
    "#     observation, reward, done, _, _ = env.last()\n",
    "#     if done:\n",
    "#         break\n",
    "\n",
    "#     action = agent.get_action(observation)\n",
    "#     env.step(action)\n",
    "    \n",
    "    # if idx % 10 == 0:\n",
    "    #     plot(env, agent, action, idx)\n",
    "# plot(env, agent, action, idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(env, 100, 64)\n",
    "env.close()\n",
    "torch.save(agents[env.agents[0]].model.state_dict(), 'modelTournament.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.14 ('tournamentVenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c34b3d126d2ccdfbc7bd258b7670b9fc8c21fb3f9a2e6d48c781c87b31d24357"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
