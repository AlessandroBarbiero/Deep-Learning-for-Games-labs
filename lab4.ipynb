{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NGzC3uqmuKZB",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Lab 4: Q-table based reinforcement learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KYeKUsX8uXSF",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "\n",
    "Solve [`FrozenLake8x8-v1`](https://www.gymlibrary.dev/environments/toy_text/frozen_lake/) using a Q-table.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hGAOGNSWyncb",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "1. Import Necessary Packages (e.g. `gym`, `numpy`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V7KHXZDxys6J",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "2. Instantiate the Environment and Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start environment\n",
    "env = gym.make('FrozenLake-v1', render_mode='human', is_slippery=False)\n",
    "\n",
    "def play_rnd(env, times=2):\n",
    "    for _ in tqdm(range(times)):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = env.action_space.sample()\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "    env.close()\n",
    "\n",
    "# play_rnd(env)\n",
    "\n",
    "# The class agent contains only the hyperparameters of the training to make more easy to divide the sections inside the notebook\n",
    "# Q-table and next methods could be placed here\n",
    "class Agent:\n",
    "    def __init__(self, qtable):\n",
    "        self.qtable = qtable\n",
    "\n",
    "    def test(self, env, total_test_episodes, max_steps):\n",
    "        env.reset()\n",
    "        rewards = []\n",
    "\n",
    "        for episode in tqdm(range(total_test_episodes)):\n",
    "            state,_ = env.reset()\n",
    "            step = 0\n",
    "            done = False\n",
    "            total_rewards = 0\n",
    "        #    print(\"****************************************************\")\n",
    "        #    print(\"EPISODE \", episode)\n",
    "\n",
    "            for step in range(max_steps):\n",
    "\n",
    "                env.render()\n",
    "                action = np.argmax(self.qtable[state,:])\n",
    "                new_state, reward, done, _, _ = env.step(action)\n",
    "                \n",
    "                total_rewards += reward\n",
    "                \n",
    "                if done:\n",
    "                    rewards.append(total_rewards)\n",
    "        #            print (\"Score\", total_rewards)\n",
    "                    break\n",
    "                state = new_state\n",
    "\n",
    "        print (\"Average episode return: \" +  str(sum(rewards)/total_test_episodes) )\n",
    "        print (\"Average win: \" +  str((sum(rewards)/total_test_episodes)*100) + \"%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QMs2BVFZywAJ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "3. Set up the QTable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_size = env.action_space.n\n",
    "state_size = env.observation_space.n\n",
    "# choose the better initialization\n",
    "initialize = \"memory\"\n",
    "\n",
    "if(initialize == \"0\"):\n",
    "    qtable = np.zeros((state_size, action_size))\n",
    "elif(initialize == \"random\"):\n",
    "    qtable = np.random.rand(state_size, action_size)\n",
    "elif(initialize == \"memory\"):\n",
    "    qtable = np.loadtxt(\"qtable_FrozenLake\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YHuDteJVy2_C",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "4. The Q-Learning algorithm training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:22<00:00,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total wins: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def train(qtable, total_episodes=50000, max_steps=100, learning_rate=0.7, gamma=0.618, decay_rate=0.01):\n",
    "    epsilon = 1.0                      # Exploration rate\n",
    "    max_epsilon = 1.0                  # Exploration probability at start\n",
    "    min_epsilon = 0.01                 # Minimum exploration probability \n",
    "\n",
    "    for episode in tqdm(range(total_episodes)):\n",
    "        # Reset the environment\n",
    "        state,_ = env.reset()\n",
    "        step = 0\n",
    "        done = False\n",
    "        total_return = 0\n",
    "        win = 0\n",
    "        for step in range(max_steps):\n",
    "            # Choose an action a in the current world state (s)\n",
    "            ## First we randomize a number\n",
    "            exp_exp_tradeoff = random.uniform(0,1)\n",
    "            \n",
    "            ## If this number > greater than epsilon --> exploitation (taking the biggest Q value for this state)\n",
    "            if exp_exp_tradeoff > epsilon:\n",
    "                action = np.argmax(qtable[state,:])\n",
    "            \n",
    "            # Else doing a random choice --> exploration\n",
    "            else:\n",
    "                action = env.action_space.sample()\n",
    "            \n",
    "            # Take the action (a) and observe the outcome state(s') and reward (r)\n",
    "            new_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "            # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "            qtable[state, action] = qtable[state, action] + learning_rate * (reward + gamma * \n",
    "                                        np.max(qtable[new_state, :]) - qtable[state, action])\n",
    "                    \n",
    "            # Our new state is state\n",
    "            state = new_state\n",
    "            \n",
    "            total_return += reward\n",
    "            # If done : finish episode\n",
    "            if done == True: \n",
    "                if total_return > 0:\n",
    "                    win += 1\n",
    "                break\n",
    "    #    print(f\"episode return {total_return}\")\n",
    "        \n",
    "        # Reduce epsilon (because we need less and less exploration)\n",
    "        epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n",
    "    print(f\"Total wins: {win}\")\n",
    "train(qtable, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mm8oigYjzFTd",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "5. Evaluate how well your agent performs\n",
    "* Render output of one episode\n",
    "* Give an average episode return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:12<00:00,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average episode return: 0.0\n",
      "Average win: 0.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(qtable)\n",
    "agent.test(env, 10, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"qtable_FrozenLake\", qtable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "6. (<i>Optional</i>) Adapt code for one of the continuous [Classical Control](https://www.gymlibrary.dev/environments/classic_control/) problems. Think/talk about how you could use our  `Model` class from last Thursday to decide actions."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOtNkS92UHFInFg+R4UDAlq",
   "name": "Reinforcement_learning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('dlgs')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "aa87fa5d0095d65c3d05fa3a4de96fff1a1d333803f7fbe0adaa457ad4e6feb1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
