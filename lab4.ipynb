{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NGzC3uqmuKZB",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Lab 4: Q-table based reinforcement learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KYeKUsX8uXSF",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "\n",
    "Solve [`FrozenLake8x8-v1`](https://www.gymlibrary.dev/environments/toy_text/frozen_lake/) using a Q-table.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hGAOGNSWyncb",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "1. Import Necessary Packages (e.g. `gym`, `numpy`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V7KHXZDxys6J",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "2. Instantiate the Environment and Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start environment\n",
    "env = gym.make('FrozenLake-v1', map_name=\"8x8\", is_slippery=True)\n",
    "\n",
    "def play_rnd(env, times=2):\n",
    "    for _ in tqdm(range(times)):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = env.action_space.sample()\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "    env.close()\n",
    "\n",
    "# play_rnd(env)\n",
    "\n",
    "# The class agent contains only the hyperparameters of the training to make more easy to divide the sections inside the notebook\n",
    "# Q-table and next methods could be placed here\n",
    "class Agent:\n",
    "    def __init__(self, qtable):\n",
    "        self.qtable = qtable\n",
    "\n",
    "    def test(self, env, total_test_episodes, max_steps):\n",
    "        env.reset()\n",
    "        rewards = []\n",
    "\n",
    "        for episode in tqdm(range(total_test_episodes)):\n",
    "            state,_ = env.reset()\n",
    "            step = 0\n",
    "            done = False\n",
    "            total_rewards = 0\n",
    "\n",
    "            for step in range(max_steps):\n",
    "\n",
    "                action = np.argmax(self.qtable[state,:])\n",
    "                new_state, reward, done, _, _ = env.step(action)\n",
    "                \n",
    "                total_rewards += reward\n",
    "                \n",
    "                if done:\n",
    "                    rewards.append(total_rewards)\n",
    "                    break\n",
    "                state = new_state\n",
    "\n",
    "        print (\"Average episode return: \" +  str(sum(rewards)/total_test_episodes) )\n",
    "        print (\"Average win: \" +  str((sum(rewards)/total_test_episodes)*100) + \"%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QMs2BVFZywAJ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "3. Set up the QTable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_size = env.action_space.n\n",
    "state_size = env.observation_space.n\n",
    "# choose the better initialization\n",
    "initialize = \"memory\"\n",
    "\n",
    "if(initialize == \"0\"):\n",
    "    qtable = np.zeros((state_size, action_size))\n",
    "elif(initialize == \"random\"):\n",
    "    qtable = np.random.rand(state_size, action_size)\n",
    "elif(initialize == \"memory\"):\n",
    "    qtable = np.loadtxt(\"qtable_FrozenLake\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YHuDteJVy2_C",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "4. The Q-Learning algorithm training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [01:32<00:00, 108.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total wins: 1063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def train(qtable, total_episodes=50000, max_steps=100, learning_rate=0.7, gamma=0.618, decay_rate=0.01):\n",
    "    epsilon = 1.0                      # Exploration rate\n",
    "    max_epsilon = 1.0                  # Exploration probability at start\n",
    "    min_epsilon = 0.01                 # Minimum exploration probability \n",
    "    win = 0\n",
    "    for episode in tqdm(range(total_episodes)):\n",
    "        # Reset the environment\n",
    "        state,_ = env.reset()\n",
    "        step = 0\n",
    "        done = False\n",
    "        total_return = 0\n",
    "        for step in range(max_steps):\n",
    "            # Choose an action a in the current world state (s)\n",
    "            ## First we randomize a number\n",
    "            exp_exp_tradeoff = random.uniform(0,1)\n",
    "            \n",
    "            ## If this number > greater than epsilon --> exploitation (taking the biggest Q value for this state)\n",
    "            if exp_exp_tradeoff > epsilon:\n",
    "                action = np.argmax(qtable[state,:])\n",
    "            \n",
    "            # Else doing a random choice --> exploration\n",
    "            else:\n",
    "                action = env.action_space.sample()\n",
    "            \n",
    "            # Take the action (a) and observe the outcome state(s') and reward (r)\n",
    "            new_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "            # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "            qtable[state, action] += learning_rate * (reward + gamma * \n",
    "                                        np.max(qtable[new_state, :]) - qtable[state, action])\n",
    "                    \n",
    "            # Our new state is state\n",
    "            state = new_state\n",
    "            \n",
    "            total_return += reward\n",
    "            # If done : finish episode\n",
    "            if done == True and total_return > 0: \n",
    "                win += 1\n",
    "                break\n",
    "    #    print(f\"episode return {total_return}\")\n",
    "        \n",
    "        # Reduce epsilon (because we need less and less exploration)\n",
    "        epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n",
    "    print(f\"Total wins: {win}\")\n",
    "train(qtable, 10000)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mm8oigYjzFTd",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "5. Evaluate how well your agent performs\n",
    "* Render output of one episode\n",
    "* Give an average episode return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 383.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average episode return: 0.18\n",
      "Average win: 18.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v1', map_name=\"8x8\", is_slippery=True) # render_mode='human' to see the game\n",
    "agent = Agent(qtable)\n",
    "agent.test(env, 100, 100)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 448.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average episode return: 0.2\n",
      "Average win: 20.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Here show some gameplay\n",
    "env = gym.make('FrozenLake-v1', map_name=\"8x8\", render_mode='human', is_slippery=True) # render_mode='human' to see the game\n",
    "agent = Agent(qtable)\n",
    "agent.test(env, 5, 100)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"qtable_FrozenLake\", qtable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "6. (<i>Optional</i>) Adapt code for one of the continuous [Classical Control](https://www.gymlibrary.dev/environments/classic_control/) problems. Think/talk about how you could use our  `Model` class from last Thursday to decide actions."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOtNkS92UHFInFg+R4UDAlq",
   "name": "Reinforcement_learning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.10.7 ('dlgsVenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "245e500c7ea10f8a0a5166b0fee45606f08f8f8cd109f252cc77539db3006236"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
